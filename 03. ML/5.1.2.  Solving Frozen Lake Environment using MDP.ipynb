{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8462f80f",
   "metadata": {},
   "source": [
    "# Solving Frozen Lake Environment using MDP\n",
    "\n",
    "MDPs are a fundamental concept in the field of artificial intelligence and reinforcement learning, and this is a project that focuses on solving the Frozen Lake problem using Markov Decision Processes (MDPs). The Frozen Lake problem is a well-known environment in which an agent navigates a grid world covered in ice and water. The agent's goal is to reach the goal state while avoiding holes in the ice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df4da1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values Converged\n",
      "q_values={0: {0: -0.31692927606953253, 1: -0.24103810646725116, 2: -0.24128134836455073, 3: -0.31692927606953253}, 1: {0: -0.31692927606953253, 1: -1.9, 2: -0.15699028418257105, 3: -0.24128134836455073}, 2: {0: -0.24128134836455073, 1: -0.06334294292112132, 2: -0.06335197065810792, 3: -0.15699028418257105}, 3: {0: -0.15699028418257105, 1: 0.04068501982455411, 2: 0.04062139353708975, 3: -0.06335197065810792}, 4: {0: -0.06335197065810792, 1: 0.15619128424863538, 2: 0.15603577920774173, 3: 0.04062139353708975}, 5: {0: 0.04062139353708975, 1: 0.2844076840876687, 2: -1.9, 3: 0.15603577920774173}, 6: {0: 0, 1: 0, 2: 0, 3: 0}, 7: {0: -1.9, 1: -1.9, 2: -0.9990889488055995, 3: -0.9990889488055995}, 8: {0: -0.24103810646725116, 1: -0.1567200154077938, 2: -1.9, 3: -0.31692927606953253}, 9: {0: 0, 1: 0, 2: 0, 3: 0}, 10: {0: -1.9, 1: -1.9, 2: 0.04068501982455411, 3: -0.15699028418257105}, 11: {0: -0.06334294292112132, 1: 0.15623471114967077, 2: 0.15619128424863538, 3: -0.06335197065810792}, 12: {0: 0.04068501982455411, 1: 0.28452149736235016, 2: 0.2844076840876687, 3: 0.04062139353708975}, 13: {0: 0.15619128424863538, 1: 0.4269209203016465, 2: 0.15657952625230048, 3: 0.15603577920774173}, 14: {0: 0.2844076840876687, 1: 0.2848353992615743, 2: -1.9, 3: -1.9}, 15: {0: 0, 1: 0, 2: 0, 3: 0}, 16: {0: -0.1567200154077938, 1: -0.06304264428247994, 2: -1.9, 3: -0.24103810646725116}, 17: {0: 0, 1: 0, 2: 0, 3: 0}, 18: {0: 0, 1: 0, 2: 0, 3: 0}, 19: {0: -1.9, 1: -1.9, 2: 0.28452149736235016, 3: 0.04068501982455411}, 20: {0: 0.15623471114967077, 1: -1.9, 2: 0.4269209203016465, 3: 0.15619128424863538}, 21: {0: 0.28452149736235016, 1: 0.5849606706877679, 2: 0.2848353992615743, 3: 0.2844076840876687}, 22: {0: 0.4269209203016465, 1: -1.9, 2: 0.15702114442784534, 3: 0.15657952625230048}, 23: {0: 0.2848353992615743, 1: 0.043117650981269975, 2: 0.15702114442784534, 3: -1.9}, 24: {0: -0.06304264428247994, 1: 0.04096508951000975, 2: 0.04102871579747408, 3: -0.1567200154077938}, 25: {0: -0.06304264428247994, 1: 0.15657316866299092, 2: 0.15661659556402632, 3: -1.9}, 26: {0: 0.04102871579747408, 1: 0.28494581337830077, 2: -1.9, 3: -1.9}, 27: {0: 0, 1: 0, 2: 0, 3: 0}, 28: {0: 0, 1: 0, 2: 0, 3: 0}, 29: {0: -1.9, 1: 0.7598125360743965, 2: -1.9, 3: 0.4269209203016465}, 30: {0: 0, 1: 0, 2: 0, 3: 0}, 31: {0: -1.9, 1: -0.05645152679910035, 2: 0.043117650981269975, 3: 0.15702114442784534}, 32: {0: 0.04096508951000975, 1: 0.15603577920774173, 2: 0.15657316866299092, 3: -0.06304264428247994}, 33: {0: 0.04096508951000975, 1: 0.2844076840876687, 2: 0.28494581337830077, 3: 0.04102871579747408}, 34: {0: 0.15657316866299092, 1: 0.4269209203016465, 2: 0.4273923825415916, 3: 0.15661659556402632}, 35: {0: 0.28494581337830077, 1: 0.5849606706877679, 2: 0.5852478860546256, 3: -1.9}, 36: {0: 0.4273923825415916, 1: 0.7598125360743965, 2: 0.7598125360743965, 3: -1.9}, 37: {0: 0.5852478860546256, 1: 0.9524079678177401, 2: -1.9, 3: 0.5849606706877679}, 38: {0: 0, 1: 0, 2: 0, 3: 0}, 39: {0: -1.9, 1: -1.9, 2: -0.05645152679910035, 3: 0.043117650981269975}, 40: {0: 0.15603577920774173, 1: 0.28192061550412173, 2: 0.2844076840876687, 3: 0.04096508951000975}, 41: {0: 0.15603577920774173, 1: 0.4242619500087229, 2: 0.4269209203016465, 3: 0.15657316866299092}, 42: {0: 0.2844076840876687, 1: 0.5822603145464881, 2: 0.5849606706877679, 3: 0.28494581337830077}, 43: {0: 0.4269209203016465, 1: 0.7573930483959951, 2: 0.7598125360743965, 3: 0.4273923825415916}, 44: {0: 0.5849606706877679, 1: 0.9508882679643607, 2: 0.9524079678177401, 3: 0.5852478860546256}, 45: {0: 0.7598125360743965, 1: 1.1631413843300258, 2: 1.1631413843300258, 3: 0.7598125360743965}, 46: {0: 0.9524079678177401, 1: 1.3927617596726038, 2: -1.9, 3: -1.9}, 47: {0: 0, 1: 0, 2: 0, 3: 0}, 48: {0: 0.28192061550412173, 1: 0.4136710415077341, 2: 0.4242619500087229, 3: 0.15603577920774173}, 49: {0: 0.28192061550412173, 1: 0.5706443737647712, 2: 0.5822603145464881, 3: 0.2844076840876687}, 50: {0: 0.4242619500087229, 1: 0.7448856793878975, 2: 0.7573930483959951, 3: 0.4269209203016465}, 51: {0: 0.5822603145464881, 1: 0.9380182158838545, 2: 0.9508882679643607, 3: 0.5849606706877679}, 52: {0: 0.7573930483959951, 1: 1.151355105430717, 2: 1.1631413843300258, 3: 0.7598125360743965}, 53: {0: 0.9508882679643607, 1: 1.3851250201255114, 2: 1.3927617596726038, 3: 0.9524079678177401}, 54: {0: 1.1631413843300258, 1: 1.6368941421369996, 2: 1.6368941421369996, 3: 1.1631413843300258}, 55: {0: 1.3927617596726038, 1: 1.9, 2: 1.6368941421369996, 3: -1.9}, 56: {0: 0.4136710415077341, 1: 0.4136710415077341, 2: 0.5706443737647712, 3: 0.28192061550412173}, 57: {0: 0.4136710415077341, 1: 0.5706443737647712, 2: 0.7448856793878975, 3: 0.4242619500087229}, 58: {0: 0.5706443737647712, 1: 0.7448856793878975, 2: 0.9380182158838545, 3: 0.5822603145464881}, 59: {0: 0.7448856793878975, 1: 0.9380182158838545, 2: 1.151355105430717, 3: 0.7573930483959951}, 60: {0: 0.9380182158838545, 1: 1.151355105430717, 2: 1.3851250201255114, 3: 0.9508882679643607}, 61: {0: 1.151355105430717, 1: 1.3851250201255114, 2: 1.6368941421369996, 3: 1.1631413843300258}, 62: {0: 1.3851250201255114, 1: 1.6368941421369996, 2: 1.9, 3: 1.3927617596726038}}\n",
      "v_values={0: -0.24103810646725116, 1: -0.15699028418257105, 2: -0.06334294292112132, 3: 0.04068501982455411, 4: 0.15619128424863538, 5: 0.2844076840876687, 6: -1, 7: -0.9990889488055995, 8: -0.1567200154077938, 9: -1, 10: 0.04068501982455411, 11: 0.15623471114967077, 12: 0.28452149736235016, 13: 0.4269209203016465, 14: 0.2848353992615743, 15: -1, 16: -0.06304264428247994, 17: -1, 18: -1, 19: 0.28452149736235016, 20: 0.4269209203016465, 21: 0.5849606706877679, 22: 0.4269209203016465, 23: 0.2848353992615743, 24: 0.04102871579747408, 25: 0.15661659556402632, 26: 0.28494581337830077, 27: -1, 28: -1, 29: 0.7598125360743965, 30: -1, 31: 0.15702114442784534, 32: 0.15657316866299092, 33: 0.28494581337830077, 34: 0.4273923825415916, 35: 0.5852478860546256, 36: 0.7598125360743965, 37: 0.9524079678177401, 38: -1, 39: 0.043117650981269975, 40: 0.2844076840876687, 41: 0.4269209203016465, 42: 0.5849606706877679, 43: 0.7598125360743965, 44: 0.9524079678177401, 45: 1.1631413843300258, 46: 1.3927617596726038, 47: -1, 48: 0.4242619500087229, 49: 0.5822603145464881, 50: 0.7573930483959951, 51: 0.9508882679643607, 52: 1.1631413843300258, 53: 1.3927617596726038, 54: 1.6368941421369996, 55: 1.9, 56: 0.5706443737647712, 57: 0.7448856793878975, 58: 0.9380182158838545, 59: 1.151355105430717, 60: 1.3851250201255114, 61: 1.6368941421369996, 62: 1.9, 63: 1}\n",
      "policy={0: 1, 1: 2, 2: 1, 3: 1, 4: 1, 5: 1, 7: 2, 8: 1, 10: 2, 11: 1, 12: 1, 13: 1, 14: 1, 16: 1, 19: 2, 20: 2, 21: 1, 22: 0, 23: 0, 24: 2, 25: 2, 26: 1, 29: 1, 31: 3, 32: 2, 33: 2, 34: 2, 35: 2, 36: 1, 37: 1, 39: 3, 40: 2, 41: 2, 42: 2, 43: 2, 44: 2, 45: 1, 46: 1, 48: 2, 49: 2, 50: 2, 51: 2, 52: 2, 53: 2, 54: 1, 55: 1, 56: 2, 57: 2, 58: 2, 59: 2, 60: 2, 61: 2, 62: 2}\n",
      "Number of wins 71\n",
      "Win Ratio: 7.100%\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "# create Environment\n",
    "\n",
    "max_iter_number = 1000\n",
    "discount_factor = 0.9\n",
    "map_size = 8\n",
    "\n",
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    desc=generate_random_map(size=map_size),\n",
    "    render_mode=\"human\",\n",
    "    is_slippery=False,\n",
    ")\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# create empty dictionary to store the policy for each state\n",
    "policy = {}\n",
    "# create a set to store the terminal states\n",
    "terminal_states = set()\n",
    "# set the goal state, which is the last cell of the grid\n",
    "goal_state = (map_size - 1) * map_size + (map_size - 1)\n",
    "\n",
    "\n",
    "# Value Iteration Algorithm\n",
    "def value_iteration():\n",
    "    # Initialize\n",
    "    v_values, q_values = {}, {}\n",
    "    convergenceTrack = [0]\n",
    "\n",
    "    # Initialize the value of each state to 0 and store the terminal states\n",
    "    for state in env.P:\n",
    "        if state == goal_state:\n",
    "            continue\n",
    "        v_values[state] = 0\n",
    "        q_values[state] = {}\n",
    "        for act in env.P[state]:\n",
    "            q_values[state][act] = 0\n",
    "            for probability, nextState, reward, isTerminalState in env.P[state][act]:\n",
    "                if (reward == 0) and isTerminalState:\n",
    "                    terminal_states.add(nextState)\n",
    "\n",
    "    # Set the value of the goal state to 1 and the terminal states to -1\n",
    "    v_values[goal_state] = 1\n",
    "    for ts in terminal_states:\n",
    "        v_values[ts] = -1\n",
    "\n",
    "    for i in range(10000):\n",
    "        # Check states in the environment\n",
    "        for state in env.P:\n",
    "            if (state not in terminal_states) and (state != goal_state):\n",
    "                # Check actions in that state\n",
    "                for act in env.P[state]:\n",
    "                    s = 0\n",
    "                    # Calculate every result of the action\n",
    "                    for probability, nextState, reward, isTerminalState in env.P[state][\n",
    "                        act\n",
    "                    ]:\n",
    "                        # Calculate the reward of each actions\n",
    "                        if (reward == 0) and isTerminalState:\n",
    "                            reward = -1\n",
    "                        elif (reward == 1) and isTerminalState:\n",
    "                            reward = 1\n",
    "                        else:\n",
    "                            # Calculate the distance to the goal\n",
    "                            x, y = nextState // map_size, nextState % map_size\n",
    "                            dist_to_goal = np.sqrt(\n",
    "                                np.power(x - (map_size - 1), 2)\n",
    "                                + np.power(y - (map_size - 1), 2)\n",
    "                            )\n",
    "                            reward = -0.1 / (1 + np.exp(-dist_to_goal))\n",
    "                        s += probability * (\n",
    "                            reward + (discount_factor * v_values[nextState])\n",
    "                        )\n",
    "                    # Update the q_value of state and action\n",
    "                    q_values[state][act] = s\n",
    "                # Update the v_value of state and policy\n",
    "                v_values[state] = max(q_values[state].values())\n",
    "                # Check convergence\n",
    "                convergenceTrack.append(np.linalg.norm(list(v_values.values())))\n",
    "                if (i > 1000) and np.isclose(\n",
    "                    convergenceTrack[-1], convergenceTrack[-2]\n",
    "                ):\n",
    "                    print(\"Values Converged\")\n",
    "                    return v_values, q_values\n",
    "    return v_values, q_values\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    v_values, q_values = value_iteration()\n",
    "    for state in env.P:\n",
    "        if (state not in terminal_states) and (state != goal_state):\n",
    "            policy[state] = max(q_values[state], key=q_values[state].get)\n",
    "\n",
    "    n_win = 0\n",
    "    print(f\"{q_values=}\")\n",
    "    print(f\"{v_values=}\")\n",
    "    print(f\"{policy=}\")\n",
    "\n",
    "    # Save the policy\n",
    "    with open(\"policy.txt\", \"w\", encoding=\"utf-8\") as inp:\n",
    "        for i in range(map_size):\n",
    "            for j in range(map_size):\n",
    "                if ((i * map_size) + j) in terminal_states:\n",
    "                    inp.write(\"☠\\t\")\n",
    "                elif ((i * map_size) + j) == goal_state:\n",
    "                    inp.write(\"🪙\\t\")\n",
    "                elif policy[(i * map_size) + j] == 0:\n",
    "                    inp.write(\"←\\t\")\n",
    "                elif policy[(i * map_size) + j] == 1:\n",
    "                    inp.write(\"↓\\t\")\n",
    "                elif policy[(i * map_size) + j] == 2:\n",
    "                    inp.write(\"→\\t\")\n",
    "                elif policy[(i * map_size) + j] == 3:\n",
    "                    inp.write(\"↑\\t\")\n",
    "            inp.write(\"\\n\")\n",
    "\n",
    "    # Test the policy\n",
    "    for r in range(max_iter_number):\n",
    "        action = policy[observation]\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if reward:\n",
    "            n_win += 1\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(f\"Number of wins {n_win}\")\n",
    "    print(f\"Win Ratio: {n_win / max_iter_number * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a8e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
