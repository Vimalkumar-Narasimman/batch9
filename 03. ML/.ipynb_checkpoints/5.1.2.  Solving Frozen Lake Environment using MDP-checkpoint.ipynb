{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8462f80f",
   "metadata": {},
   "source": [
    "# Solving Frozen Lake Environment using MDP\n",
    "\n",
    "MDPs are a fundamental concept in the field of artificial intelligence and reinforcement learning, and this is a project that focuses on solving the Frozen Lake problem using Markov Decision Processes (MDPs). The Frozen Lake problem is a well-known environment in which an agent navigates a grid world covered in ice and water. The agent's goal is to reach the goal state while avoiding holes in the ice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4da1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values Converged\n",
      "q_values={0: {0: -0.3175467425695185, 1: -0.38151653688817616, 2: -0.24172418035612442, 3: -0.3175467425695185}, 1: {0: -0.3175467425695185, 1: -1.9, 2: -0.15748231972876403, 3: -0.24172418035612442}, 2: {0: -0.24172418035612442, 1: -0.06388964908355797, 2: -0.06389867682054458, 3: -0.15748231972876403}, 3: {0: -0.15748231972876403, 1: 0.040077568532957825, 2: 0.03996901669803453, 3: -0.06389867682054458}, 4: {0: -0.06389867682054458, 1: 0.15546642109412956, 2: -0.06395902144292903, 3: 0.03996901669803453}, 5: {0: 0.03996901669803453, 1: -1.9, 2: -1.9, 3: -0.06395902144292903}, 6: {0: 0, 1: 0, 2: 0, 3: 0}, 7: {0: -1.9, 1: 0.28574164067184027, 2: 0.15725858172409635, 3: 0.15725858172409635}, 8: {0: -0.38151653688817616, 1: -0.31280716031993266, 2: -1.9, 3: -0.3175467425695185}, 9: {0: 0, 1: 0, 2: 0, 3: 0}, 10: {0: -1.9, 1: -1.9, 2: 0.040077568532957825, 3: -0.15748231972876403}, 11: {0: -0.06388964908355797, 1: 0.15555976527011936, 2: 0.15546642109412956, 3: -0.06389867682054458}, 12: {0: 0.040077568532957825, 1: 0.2837160938573437, 2: -1.9, 3: 0.03996901669803453}, 13: {0: 0, 1: 0, 2: 0, 3: 0}, 14: {0: -1.9, 1: 0.5822603145464881, 2: 0.28574164067184027, 3: -1.9}, 15: {0: 0.4242619500087229, 1: 0.4283270870624186, 2: 0.28574164067184027, 3: 0.15725858172409635}, 16: {0: -0.31280716031993266, 1: -0.23647280529596756, 2: -0.38143603124208836, 3: -0.38151653688817616}, 17: {0: -0.31280716031993266, 1: -0.3127517449765204, 2: -1.9, 3: -1.9}, 18: {0: 0, 1: 0, 2: 0, 3: 0}, 19: {0: -1.9, 1: 0.28377155749618194, 2: 0.2837160938573437, 3: 0.040077568532957825}, 20: {0: 0.15555976527011936, 1: 0.426026027518306, 2: 0.42581305910853284, 3: 0.15546642109412956}, 21: {0: 0.2837160938573437, 1: 0.5837297138065305, 2: 0.5822603145464881, 3: -1.9}, 22: {0: 0.42581305910853284, 1: 0.7573930483959951, 2: 0.4283270870624186, 3: 0.4242619500087229}, 23: {0: 0.5822603145464881, 1: 0.5862864466333223, 2: 0.4283270870624186, 3: 0.28574164067184027}, 24: {0: -0.23647280529596756, 1: -0.15167146310640103, 2: -0.3127517449765204, 3: -0.31280716031993266}, 25: {0: -0.23647280529596756, 1: -1.9, 2: -1.9, 3: -0.38143603124208836}, 26: {0: 0, 1: 0, 2: 0, 3: 0}, 27: {0: -1.9, 1: 0.15606368683899222, 2: 0.426026027518306, 3: 0.15555976527011936}, 28: {0: 0.28377155749618194, 1: -1.9, 2: 0.5837297138065305, 3: 0.2837160938573437}, 29: {0: 0.426026027518306, 1: 0.758444806206355, 2: 0.7573930483959951, 3: 0.42581305910853284}, 30: {0: 0.5837297138065305, 1: 0.9508882679643607, 2: 0.5862864466333223, 3: 0.5822603145464881}, 31: {0: 0.7573930483959951, 1: 0.7605420284856813, 2: 0.5862864466333223, 3: 0.4283270870624186}, 32: {0: -0.15167146310640103, 1: -0.057467445355243285, 2: -1.9, 3: -0.23647280529596756}, 33: {0: 0, 1: 0, 2: 0, 3: 0}, 34: {0: -1.9, 1: 0.16333669244610943, 2: 0.15606368683899222, 3: -1.9}, 35: {0: 0.0472956922923668, 1: -1.9, 2: -1.9, 3: 0.28377155749618194}, 36: {0: 0, 1: 0, 2: 0, 3: 0}, 37: {0: -1.9, 1: -1.9, 2: 0.9508882679643607, 3: 0.5837297138065305}, 38: {0: 0.758444806206355, 1: 1.1631413843300258, 2: 0.7605420284856813, 3: 0.7573930483959951}, 39: {0: 0.9508882679643607, 1: -1.9, 2: 0.7605420284856813, 3: 0.5862864466333223}, 40: {0: -0.057467445355243285, 1: 0.04688209890857502, 2: 0.04718187901768536, 3: -0.15167146310640103}, 41: {0: -0.057467445355243285, 1: 0.16310804268033768, 2: 0.16333669244610943, 3: -1.9}, 42: {0: 0.04718187901768536, 1: 0.29208930640383785, 2: -1.9, 3: 0.0472956922923668}, 43: {0: 0, 1: 0, 2: 0, 3: 0}, 44: {0: -1.9, 1: -1.9, 2: -1.9, 3: -1.9}, 45: {0: 0, 1: 0, 2: 0, 3: 0}, 46: {0: -1.9, 1: 1.3927617596726038, 2: -1.9, 3: 0.9508882679643607}, 47: {0: 0, 1: 0, 2: 0, 3: 0}, 48: {0: 0.04688209890857502, 1: -1.9, 2: 0.16310804268033768, 3: -0.057467445355243285}, 49: {0: 0.04688209890857502, 1: 0.29089810780733116, 2: 0.29208930640383785, 3: 0.04718187901768536}, 50: {0: 0.16310804268033768, 1: 0.43405649499074184, 2: 0.43498081712638376, 3: 0.16333669244610943}, 51: {0: 0.29208930640383785, 1: 0.5926524554425704, 2: -1.9, 3: -1.9}, 52: {0: 0, 1: 0, 2: 0, 3: 0}, 53: {0: -1.9, 1: 0.9587475380992349, 2: 1.3927617596726038, 3: -1.9}, 54: {0: 1.1631413843300258, 1: -1.9, 2: 1.6368941421369996, 3: 1.1631413843300258}, 55: {0: 1.3927617596726038, 1: 1.9, 2: 1.6368941421369996, 3: -1.9}, 56: {0: 0, 1: 0, 2: 0, 3: 0}, 57: {0: -1.9, 1: 0.29089810780733116, 2: 0.43405649499074184, 3: 0.16310804268033768}, 58: {0: 0.29089810780733116, 1: 0.43405649499074184, 2: 0.5926524554425704, 3: 0.29208930640383785}, 59: {0: 0.43405649499074184, 1: 0.5926524554425704, 2: 0.767615371607068, 3: 0.43498081712638376}, 60: {0: 0.5926524554425704, 1: 0.767615371607068, 2: 0.9587475380992349, 3: -1.9}, 61: {0: 0.767615371607068, 1: 0.9587475380992349, 2: -1.9, 3: 1.1631413843300258}, 62: {0: 0, 1: 0, 2: 0, 3: 0}}\n",
      "v_values={0: -0.24172418035612442, 1: -0.15748231972876403, 2: -0.06388964908355797, 3: 0.040077568532957825, 4: 0.15546642109412956, 5: 0.03996901669803453, 6: -1, 7: 0.28574164067184027, 8: -0.31280716031993266, 9: -1, 10: 0.040077568532957825, 11: 0.15555976527011936, 12: 0.2837160938573437, 13: -1, 14: 0.5822603145464881, 15: 0.4283270870624186, 16: -0.23647280529596756, 17: -0.3127517449765204, 18: -1, 19: 0.28377155749618194, 20: 0.426026027518306, 21: 0.5837297138065305, 22: 0.7573930483959951, 23: 0.5862864466333223, 24: -0.15167146310640103, 25: -0.23647280529596756, 26: -1, 27: 0.426026027518306, 28: 0.5837297138065305, 29: 0.758444806206355, 30: 0.9508882679643607, 31: 0.7605420284856813, 32: -0.057467445355243285, 33: -1, 34: 0.16333669244610943, 35: 0.28377155749618194, 36: -1, 37: 0.9508882679643607, 38: 1.1631413843300258, 39: 0.9508882679643607, 40: 0.04718187901768536, 41: 0.16333669244610943, 42: 0.29208930640383785, 43: -1, 44: -1.9, 45: -1, 46: 1.3927617596726038, 47: -1, 48: 0.16310804268033768, 49: 0.29208930640383785, 50: 0.43498081712638376, 51: 0.5926524554425704, 52: -1, 53: 1.3927617596726038, 54: 1.6368941421369996, 55: 1.9, 56: -1, 57: 0.43405649499074184, 58: 0.5926524554425704, 59: 0.767615371607068, 60: 0.9587475380992349, 61: 1.1631413843300258, 62: -1, 63: 1}\n",
      "policy={0: 2, 1: 2, 2: 1, 3: 1, 4: 1, 5: 0, 7: 1, 8: 1, 10: 2, 11: 1, 12: 1, 14: 1, 15: 1, 16: 1, 17: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 0, 27: 2, 28: 2, 29: 1, 30: 1, 31: 1, 32: 1, 34: 1, 35: 3, 37: 2, 38: 1, 39: 0, 40: 2, 41: 2, 42: 1, 44: 0, 46: 1, 48: 2, 49: 2, 50: 2, 51: 1, 53: 2, 54: 2, 55: 1, 57: 2, 58: 2, 59: 2, 60: 2, 61: 3}\n",
      "Number of wins 0\n",
      "Win Ratio: 0.000%\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "# create Environment\n",
    "\n",
    "max_iter_number = 10\n",
    "discount_factor = 0.9\n",
    "map_size = 8\n",
    "\n",
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    desc=generate_random_map(size=map_size),\n",
    "    render_mode=\"human\",\n",
    "    is_slippery=False,\n",
    ")\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# create empty dictionary to store the policy for each state\n",
    "policy = {}\n",
    "# create a set to store the terminal states\n",
    "terminal_states = set()\n",
    "# set the goal state, which is the last cell of the grid\n",
    "goal_state = (map_size - 1) * map_size + (map_size - 1)\n",
    "\n",
    "\n",
    "# Value Iteration Algorithm\n",
    "def value_iteration():\n",
    "    # Initialize\n",
    "    v_values, q_values = {}, {}\n",
    "    convergenceTrack = [0]\n",
    "\n",
    "    # Initialize the value of each state to 0 and store the terminal states\n",
    "    for state in env.P:\n",
    "        if state == goal_state:\n",
    "            continue\n",
    "        v_values[state] = 0\n",
    "        q_values[state] = {}\n",
    "        for act in env.P[state]:\n",
    "            q_values[state][act] = 0\n",
    "            for probability, nextState, reward, isTerminalState in env.P[state][act]:\n",
    "                if (reward == 0) and isTerminalState:\n",
    "                    terminal_states.add(nextState)\n",
    "\n",
    "    # Set the value of the goal state to 1 and the terminal states to -1\n",
    "    v_values[goal_state] = 1\n",
    "    for ts in terminal_states:\n",
    "        v_values[ts] = -1\n",
    "\n",
    "    for i in range(10000):\n",
    "        # Check states in the environment\n",
    "        for state in env.P:\n",
    "            if (state not in terminal_states) and (state != goal_state):\n",
    "                # Check actions in that state\n",
    "                for act in env.P[state]:\n",
    "                    s = 0\n",
    "                    # Calculate every result of the action\n",
    "                    for probability, nextState, reward, isTerminalState in env.P[state][\n",
    "                        act\n",
    "                    ]:\n",
    "                        # Calculate the reward of each actions\n",
    "                        if (reward == 0) and isTerminalState:\n",
    "                            reward = -1\n",
    "                        elif (reward == 1) and isTerminalState:\n",
    "                            reward = 1\n",
    "                        else:\n",
    "                            # Calculate the distance to the goal\n",
    "                            x, y = nextState // map_size, nextState % map_size\n",
    "                            dist_to_goal = np.sqrt(\n",
    "                                np.power(x - (map_size - 1), 2)\n",
    "                                + np.power(y - (map_size - 1), 2)\n",
    "                            )\n",
    "                            reward = -0.1 / (1 + np.exp(-dist_to_goal))\n",
    "                        s += probability * (\n",
    "                            reward + (discount_factor * v_values[nextState])\n",
    "                        )\n",
    "                    # Update the q_value of state and action\n",
    "                    q_values[state][act] = s\n",
    "                # Update the v_value of state and policy\n",
    "                v_values[state] = max(q_values[state].values())\n",
    "                # Check convergence\n",
    "                convergenceTrack.append(np.linalg.norm(list(v_values.values())))\n",
    "                if (i > 1000) and np.isclose(\n",
    "                    convergenceTrack[-1], convergenceTrack[-2]\n",
    "                ):\n",
    "                    print(\"Values Converged\")\n",
    "                    return v_values, q_values\n",
    "    return v_values, q_values\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    v_values, q_values = value_iteration()\n",
    "    for state in env.P:\n",
    "        if (state not in terminal_states) and (state != goal_state):\n",
    "            policy[state] = max(q_values[state], key=q_values[state].get)\n",
    "\n",
    "    n_win = 0\n",
    "    print(f\"{q_values=}\")\n",
    "    print(f\"{v_values=}\")\n",
    "    print(f\"{policy=}\")\n",
    "\n",
    "    # Save the policy\n",
    "    with open(\"policy.txt\", \"w\", encoding=\"utf-8\") as inp:\n",
    "        for i in range(map_size):\n",
    "            for j in range(map_size):\n",
    "                if ((i * map_size) + j) in terminal_states:\n",
    "                    inp.write(\"☠\\t\")\n",
    "                elif ((i * map_size) + j) == goal_state:\n",
    "                    inp.write(\"🪙\\t\")\n",
    "                elif policy[(i * map_size) + j] == 0:\n",
    "                    inp.write(\"←\\t\")\n",
    "                elif policy[(i * map_size) + j] == 1:\n",
    "                    inp.write(\"↓\\t\")\n",
    "                elif policy[(i * map_size) + j] == 2:\n",
    "                    inp.write(\"→\\t\")\n",
    "                elif policy[(i * map_size) + j] == 3:\n",
    "                    inp.write(\"↑\\t\")\n",
    "            inp.write(\"\\n\")\n",
    "\n",
    "    # Test the policy\n",
    "    for r in range(max_iter_number):\n",
    "        action = policy[observation]\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if reward:\n",
    "            n_win += 1\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(f\"Number of wins {n_win}\")\n",
    "    print(f\"Win Ratio: {n_win / max_iter_number * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a8e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
