{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f1e9e68",
   "metadata": {},
   "source": [
    "<b>Assignment</b>\n",
    "* Assignment_Attrition\n",
    "\n",
    "<b>Objective</b>\n",
    "* Analyzing and Building models for Predicting Attrition.\n",
    "\n",
    "<b>Dataset</b>\n",
    "* The dataset, Attrition.csv, is adapted from Kaggle\n",
    "\n",
    "<b>Work to be done</b>\n",
    "1.\tWrite a Data Science Proposal for achieving the objective mentioned. Clearly describe the business understanding\n",
    "2.\tPerform exploratory analysis of the data and describe your understanding of the data.\n",
    "3.\tPerform data wrangling / Pre-Processing to improve outcomes E.g., missing data, normalization, discretization, etc.\n",
    "4.\tIs Feature Engineering required for the problem statement? Describe the technique you would adopt and justify the same\n",
    "5.\tPropose two feature selection techniques and compare the same\n",
    "6.\tPlot top 6 features that help in predicting\n",
    "7.\tProvide a high-level description of Machine Learning models – Logistic regression and Decision tree to predict.\n",
    "8.\tCompare the performance of the two classifiers – Logistic regression and Decision tree to predict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5fec4",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "\n",
    "Employee attrition is predictable under stable circumstances, wherein a set pattern can be deduced from certain parameters influencing the employee and the organization at all times. \n",
    "\n",
    "However, who is going to leave, when and why, can be answered based on analytical models developed as a result of data analysis.\n",
    "\n",
    "On a basic level, the model works by clustering/ classifying employee profiles based on various attributes such as age, sex, marital status, education level, work experience, distance from hometown, etc. and generates various levels of risk of attrition. Occasionally, other parameters like performance over the years, pay raise, work batch, educational institution are also taken into consideration.\n",
    "\n",
    "However, the accuracy of the model is directly proportional to the selection of parameters, which in turn, leads to the generation of the ‘type’ of predictive model most suitable for the organisation.\n",
    "\n",
    "For instance, ‘logistic model’ produces scorecards for employees based on their predicted ‘attrition risk’ parameters; Likewise, ‘decision trees’ model evaluate loss based on factors like gini index, information gain and variation reduction. \n",
    "\n",
    "<b>Methodology:</b>\n",
    "\n",
    "* EDA: Perform exploratory analysis of the data\n",
    "* Preprocessing the collected data: Perform data wrangling / Pre-Processing to improve outcomes\n",
    "* Analyzing the dataset: The most important features that push an employee to leave the organization are detected\n",
    "* Balancing the dataset: Since the dataset is not already balanced, it is necessary to be equalized\n",
    "* Building the predictive model: The suitable configuration for the model is selected to increase the prediction accuracy (Logistic regression and Decision tree to predict)\n",
    "* Validating the model: Compare the performance of the two classifiers – Logistic regression and Decision tree to predict\n",
    "\n",
    "Predictive Attrition Model helps in not only taking preventive measures but also into making better hiring decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cb03f",
   "metadata": {},
   "source": [
    "<b>Import the required libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#preprocess.\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "#model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710491d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e514a",
   "metadata": {},
   "source": [
    "</b>Read the uploaded 'Final dataset Attrition-1.csv' as dataframe </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36806d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Final dataset Attrition-1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de2c173",
   "metadata": {},
   "source": [
    "#### Analysing the dataset\n",
    "\n",
    "Print the shape, null check and datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5deb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {:,} rows and {} columns in the data.\".format(df.shape[0], df.shape[1]))\n",
    "print(\"There are {} missing values in the data.\".format(df.isnull().sum().sum()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64abd9b",
   "metadata": {},
   "source": [
    "#### Getting information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa75ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb455c",
   "metadata": {},
   "source": [
    "Variables type:\n",
    "\n",
    "Numeric variables:\n",
    "Related to personal information: Age, DistanceFromHome\n",
    "Related to income: MonthlyIncome, PercentSalaryHike, StockOptionLevel\n",
    "Related to time in company: TotalWorkingYears, YearsAtCompany, YearsSinceLastPromotion, YearsWithCurrManager\n",
    "other: JobInvolvement, JobLevel, JobSatisfaction, PerformanceRating, TrainingTimesLastYear, Leaves, Absenteeism\n",
    "Categorical variables:\n",
    "Binary variables: attrition(target variable), gender, OverTime, Mode_of_work, Work_accident, Job_mode\n",
    "Nominal variables: Department, Higher_Education, JobRole, MaritalStatus\n",
    "Ordinal variables:\n",
    "Ordinal regarding satisfaction and performance : JobInvolvement, JobSatisfaction, PerformanceRating, Work_accident, Leaves, BusinessTravel, JobLevel, StockOptionLevel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c98073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d92e5",
   "metadata": {},
   "source": [
    "#### Summary Statistics of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad85410",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=df.select_dtypes(include=object).columns.tolist()\n",
    "cat_df=pd.DataFrame(df[cat_cols].melt(var_name='column_name', value_name='value_count').value_counts())\\\n",
    "    .rename(columns={0: 'count'})\\\n",
    "    .sort_values(by=['column_name', 'count'])\n",
    "#display(df.select_dtypes(include=object).describe())\n",
    "display(cat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cf710",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "   * Remove unneeded features \n",
    "   * Feature Detection\n",
    "   * Detecting Outliers\n",
    "   * Preparing Dataset\n",
    "   * Transform some of the binary variables into a 1/0 format.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f127d",
   "metadata": {},
   "source": [
    "Finding Columns having only one Unique Values and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_cols = list()\n",
    "\n",
    "for col in df.columns:\n",
    "    if len(df[col].unique()) == 1:\n",
    "        redundant_cols.append(col)\n",
    "\n",
    "redundant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e81faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(redundant_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a99a2",
   "metadata": {},
   "source": [
    "### Feature engineering and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683acdf",
   "metadata": {},
   "source": [
    "The feature Selection is one of the main steps of the preprocessing phase as the features which we choose directly effects the model performance. While some of the features seem to be less useful in terms of the context; others seem to equally useful. The better features we use the better our model will perform.\n",
    "\n",
    "We can also use the Recusrive Feature Elimination technique (a wrapper method) to choose the desired number of most important features. The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n",
    "\n",
    "It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n",
    "\n",
    "We can use it directly from the scikit library by importing the RFE module or function provided by the scikit. But note that since it tries different combinations or the subset of features;it is quite computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a809cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['BusinessTravel', 'NumCompaniesWorked', 'StockOptionLevel','TrainingTimesLastYear'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c604ca4",
   "metadata": {},
   "source": [
    "#### Plotting the Features against the top 'Target' variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc221364",
   "metadata": {},
   "source": [
    "<b>1. Age</b>\n",
    "\n",
    "An Age is a continuous quantity and therefore we can plot it against the Attrition using a boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df,y='Age',x='Attrition',kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83429896",
   "metadata": {},
   "source": [
    "The median as well the maximum age of the peole with '0' attrition is higher than that of the '1' category. This shows that peole with higher age have lesser tendency to leave the organisation which makes sense as they may have settled in the organisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c52ee",
   "metadata": {},
   "source": [
    "<b>2. Job Level</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efa385",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df,kind='count',x='Attrition',col='JobLevel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba5a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(columns=[df.Attrition],index=[df.JobLevel],margins=True,normalize='index') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45157a6",
   "metadata": {},
   "source": [
    "People in Joblevel 4 have a very high percent for a '0' and a low percent for a '1'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c05c69",
   "metadata": {},
   "source": [
    "<b>3. Monthly Income</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84265a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df,kind='bar',x='Attrition',y='MonthlyIncome')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da54483",
   "metadata": {},
   "source": [
    "The average income for '0' class is quite higher and it is obvious as those earning well will certainly not be willing to exit the organisation. Similarly those employees who are probably not earning well will certainly want to change the company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab94316",
   "metadata": {},
   "source": [
    "<b>4. Job Satisfaction</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df,kind='count',x='Attrition',col='JobSatisfaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(columns=[df.Attrition],index=[df.JobSatisfaction],margins=True,normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd30277",
   "metadata": {},
   "source": [
    "The above plot shows an interesting trend. For higher values of job satisfaction( ie more a person is satisfied with his job) lesser percent of them say a '1' which is quite obvious as highly contented workers will obvioulsy not like to leave the organisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745112b1",
   "metadata": {},
   "source": [
    "<b>5. Salary Hike </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f994768",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df,kind='count',x='Attrition',col='PercentSalaryHike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea989ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(columns=[df.Attrition],index=[df.PercentSalaryHike],margins=True,normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83462a67",
   "metadata": {},
   "source": [
    "From the above, the hike between 11 and 18 having very high percent for a '0' and a low percent for a '1'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bcc25",
   "metadata": {},
   "source": [
    "<b>6. Performance Rating</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f36eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df,kind='count',x='Attrition',col='PerformanceRating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(columns=[df.Attrition],index=[df.PerformanceRating],margins=True,normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87a998",
   "metadata": {},
   "source": [
    "From the above, both rating having same attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459a3e1",
   "metadata": {},
   "source": [
    "#### Outliers Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corelation matrix.\n",
    "cor_mat= df.corr()\n",
    "mask = np.array(cor_mat)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(30,12)\n",
    "sns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f5bdf8",
   "metadata": {},
   "source": [
    "<b>SOME INFERENCES FROM THE ABOVE HEATMAP</b>\n",
    "\n",
    "1. Self relation ie of a feature to itself is equal to 1 as expected.\n",
    "\n",
    "2. JobLevel is highly related to Age as expected as aged employees will generally tend to occupy higher positions in the company.\n",
    "\n",
    "3. MonthlyIncome is very strongly related to joblevel as expected as senior employees will definately earn more.\n",
    "\n",
    "4. PerformanceRating is highly related to PercentSalaryHike which is quite obvious.\n",
    "\n",
    "5. Also note that TotalWorkingYears is highly related to JobLevel which is expected as senior employees must have worked for a larger span of time.\n",
    "\n",
    "6. YearsWithCurrManager is highly related to YearsAtCompany.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8be163",
   "metadata": {},
   "source": [
    "Note that we can drop some highly corelated features as they add redundancy to the model but since the corelation is very less in genral let us keep all the features for now. In case of highly corelated features we can use something like Principal Component Analysis(PCA) to reduce our feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(data, col):\n",
    "    Q1 = np.nanpercentile(data[col], 25, interpolation= 'midpoint')\n",
    "    Q3 = np.nanpercentile(data[col], 75, interpolation= 'midpoint')\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "\n",
    "    ls = df.index[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898ade2",
   "metadata": {},
   "source": [
    "Getting a feature list of int64 data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lst = list()\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'int64':\n",
    "        features_lst.append(col)\n",
    "\n",
    "features_lst  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceeccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in features_lst:\n",
    "    if len(detect_outliers(df, col_name)) != 0:\n",
    "        print(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.5,5))\n",
    "plt.scatter(df.Age, df.TotalWorkingYears, )\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Total Working Years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c711a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6.5,5))\n",
    "plt.scatter(df.MonthlyIncome, df.TotalWorkingYears, )\n",
    "plt.xlabel('Monthly Income')\n",
    "plt.ylabel('Total Working Years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdaa8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.5,5))\n",
    "plt.scatter(df.Age, df.MonthlyIncome,)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Monthly Income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d496ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.5,5))\n",
    "plt.scatter(df.JobLevel, df.MonthlyIncome,)\n",
    "plt.xlabel('Job Level')\n",
    "plt.ylabel('Monthly Income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['JobLevel'].corr(df['MonthlyIncome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43940b24",
   "metadata": {},
   "source": [
    "As the correlation between the two columns is very high, we can simply drop one column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18575fe1",
   "metadata": {},
   "source": [
    "#### Remove collinear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for removing correlated variables\n",
    "threshold = 0.8\n",
    "\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = df.corr().abs()\n",
    "corr_matrix.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "print('There are %d columns to remove :' % (len(to_drop)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ff7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = to_drop)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b826b",
   "metadata": {},
   "source": [
    "### Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40226e4",
   "metadata": {},
   "source": [
    "Before feeding our data into a ML model we first need to prepare the data. This includes encoding all the categorical features (either LabelEncoding or the OneHotEncoding) as the model expects the features to be in numerical form. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba2fe8a",
   "metadata": {},
   "source": [
    "<b>Feature Encoding</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ce230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(feature):\n",
    "    le=LabelEncoder()\n",
    "    df[feature]=le.fit_transform(df[feature])\n",
    "    print(le.classes_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2de827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df=df.select_dtypes(include='object')\n",
    "cat_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_df.columns:\n",
    "    transform(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a10088",
   "metadata": {},
   "source": [
    "<b>Feature Selection</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('Attrition',axis=1)\n",
    "Y=df['Attrition']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619e38a",
   "metadata": {},
   "source": [
    "<b>Splitting the data into training and validation sets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94593f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "print(\"Training split input- \", x_train.shape)\n",
    "print(\"Testing split input- \", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf911a",
   "metadata": {},
   "source": [
    "### High-level description of Machine Learning models – Logistic regression and Decision tree to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16573f",
   "metadata": {},
   "source": [
    "<b>Logistic regression</b> is used to predict the class (or category) of individuals based on one or multiple predictor variables (x). It is used to model a binary outcome, that is a variable, which can have only two possible values: 0 or 1, yes or no.\n",
    "\n",
    "In logistic regression, a logit transformation is applied on the odds—that is, the probability of success divided by the probability of failure. This is also commonly known as the log odds, or the natural logarithm of odds, and this logistic function is represented by the following formulas:\n",
    "\n",
    "Logit(pi) = 1/(1+ exp(-pi))\n",
    "\n",
    "ln(pi/(1-pi)) = Beta_0 + Beta_1*X_1 + … + B_k*K_k\n",
    "\n",
    "\n",
    "<b>Decision trees</b> tend to be the method of choice for predictive modeling because they are relatively easy to understand and are also very effective. The basic goal of a decision tree is to split a population of data into smaller segments. There are two stages to prediction. The first stage is training the model—this is where the tree is built, tested, and optimized by using an existing collection of data. In the second stage, you actually use the model to predict an unknown outcome\n",
    "\n",
    "A decision tree is a commonly used classification model, which is a flowchart-like tree structure. In a decision tree, each internal node (non-leaf node) denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (or terminal node) holds a class label. The topmost node in a tree is the root node. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb53a2a",
   "metadata": {},
   "source": [
    "### Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ecbb93",
   "metadata": {},
   "source": [
    "For better performance we will do the <b>feature scaling</b> ie bringing all the features onto the same scale by using the StandardScaler provided in the scikit library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(copy = True)\n",
    "Unscaled_x = scaler.fit_transform(x_train)\n",
    "\n",
    "x_scaled = pd.DataFrame(Unscaled_x)\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unscaled_x_test = scaler.fit_transform(x_test)\n",
    "\n",
    "x_test_scaled = pd.DataFrame(Unscaled_x_test)\n",
    "x_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a93a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "# Fit the model on the trainng data.\n",
    "lr.fit(x_scaled, y_train)\n",
    "y_pred = lr.predict(x_test_scaled)\n",
    "report = classification_report(\n",
    "                        digits= 6,\n",
    "                        y_true= y_test, \n",
    "                        y_pred= y_pred)\n",
    "print(\"Accuracy\", round(accuracy_score(y_pred, y_test, normalize=True, sample_weight=None)*100,2))\n",
    "print(report)\n",
    "pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "lrScore = lr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9367b4a9",
   "metadata": {},
   "source": [
    "We have applied logistic model to the data getting an accuracy of 86.05%. As it is clear from our model that we are getting low value of recall for true value of attrition ,i.e., we are not getting enough of the relevant information from the data. The retrieved model shows high senstivity but low specifity.\n",
    "\n",
    "Now ,we will try a new model with few less relevant features trimmed out from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1c06d",
   "metadata": {},
   "source": [
    "<b>Applying Recursive Feature Elimination (RFE) for feature selection</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(lr)\n",
    "rfe = rfe.fit(x_train, y_train)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2714168b",
   "metadata": {},
   "source": [
    "Transforming our data to desired no. of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =rfe.transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b634221",
   "metadata": {},
   "source": [
    "Calculating accuracy of our modified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c040cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = rfe.transform(x_test)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the trainng data.\n",
    "lr.fit(x, Y)\n",
    "y_pred = lr.predict(x_test)\n",
    "report = classification_report(\n",
    "                        digits= 6,\n",
    "                        y_true= y_test, \n",
    "                        y_pred= y_pred)\n",
    "print(\"Accuracy\", round(accuracy_score(y_pred, y_test, normalize=True, sample_weight=None)*100,2))\n",
    "print(report)\n",
    "pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "lrScore = lr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb0f78",
   "metadata": {},
   "source": [
    "After applying Recursive Feature Elimination (RFE) for feature selection, there is no inprovement in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7cb1f2",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3b48c",
   "metadata": {},
   "source": [
    "Without apply feature scaling for decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('Attrition',axis=1)\n",
    "Y=df['Attrition']\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "print(\"Training split input- \", x_train.shape)\n",
    "print(\"Testing split input- \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "def applyDecisionTree(X_train, y_train, X_test, y_test):\n",
    "    # Defining the decision tree algorithm\n",
    "    dtree.fit(X_train, y_train)\n",
    "    \n",
    "    score = dtree.score(X_test,y_test)\n",
    "    rounded_score = round(score * 100, 2)\n",
    "    print(\"Accuracy=\" , rounded_score)\n",
    "    \n",
    "    # Predicting the values of test data\n",
    "    y_pred = dtree.predict(X_test)\n",
    "    print(\"Classification report - \\n\", classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6863a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "applyDecisionTree(x_train, y_train, x_test, y_test)\n",
    "dtScore = dtree.score(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa56ce",
   "metadata": {},
   "source": [
    "Although overall accuracy of classification is good. Precision and accuracy of class 1 is very low indicating Class Label Imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539d0f8",
   "metadata": {},
   "source": [
    "#### Class Imbalance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the dataset shape\n",
    "print (\"Dataset Length: \", len(X))\n",
    "print (\"Dataset Shape: \", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d82df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Attrition.value_counts()/1470"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f63a5b",
   "metadata": {},
   "source": [
    "About 84 % of data are of class label 0 and only 16 % of data are of class label 1. This creates Class Imbalance. It is necessary to remove because even if we create a classifier which everytime predicts Attrition as 'No' will also achieve an overall accuracy of 84%, which is meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15788cf5",
   "metadata": {},
   "source": [
    "#### Method 1: UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = df.Attrition.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df[df['Attrition'] == 0]\n",
    "df_class_1 = df[df['Attrition'] == 1]\n",
    "\n",
    "print (\"df_class_0 Shape: \", df_class_0.shape)\n",
    "print (\"df_class_1 Shape: \", df_class_1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_under = pd.concat([df_class_0_under, df_class_1], axis= 0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_under.Attrition.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_under.drop('Attrition',axis=1)\n",
    "Y = df_under['Attrition']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a31c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in training Data\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd25da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "applyDecisionTree(x_train, y_train, x_test, y_test)\n",
    "dtScore = dtree.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8079c4d",
   "metadata": {},
   "source": [
    "After Undersampling, f1-score for minority class 1 improved. Precision and Recall also got improved. Score for class 0 reduced but that's ok. We have more generalized classifier which classifies both classes with similar prediction score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e800c05",
   "metadata": {},
   "source": [
    "#### Method 2: OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d58cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_1.sample(count_class_0, replace=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8db6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef56bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "print('Random over-sampling:')\n",
    "print(df_over.Attrition.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42740d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_over.drop('Attrition', axis=1)\n",
    "Y = df_over['Attrition']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22553182",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174840ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "applyDecisionTree(x_train, y_train, x_test, y_test)\n",
    "dtScore = dtree.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82c597",
   "metadata": {},
   "source": [
    "After Over-sampling, f1-score for minority class 1 improved to a great extent. Precision and Recall also got improved significantly. Score for class 0 is also not reduced here. We have more generalized classifier which classifies both classes with similar prediction score. Overall accuracy got improved\n",
    "\n",
    "Overall accuracy is increased and precision and recall is also improved. We will now use the OVERSAMPLING sampled X and Y to predict for further models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18038acc",
   "metadata": {},
   "source": [
    "### Compare the performance of the two classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af23a0ec",
   "metadata": {},
   "source": [
    "#### Model Scores (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores={'Logistic Regression':lrScore,\n",
    "              'Decision tree':dtScore\n",
    "             }\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6d210",
   "metadata": {},
   "source": [
    "#### Model Comparison\n",
    "Based on the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare=pd.DataFrame(model_scores,index=['accuracy'])\n",
    "model_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f5e5d6",
   "metadata": {},
   "source": [
    "we can see that Decision tree is having the best accuracy over Logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9601f",
   "metadata": {},
   "source": [
    "#### Visualize the accuracy of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare.T.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49ca68",
   "metadata": {},
   "source": [
    "#### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad3409e",
   "metadata": {},
   "source": [
    "We can see that Decision tree has ~6% better accuracy than Logistic regression but Decision tree is an Oversampling model hence we will select Logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
